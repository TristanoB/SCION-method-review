{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed89b3b",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51e181",
   "metadata": {},
   "source": [
    "This notebook is structured in the following manner : \n",
    "- implementation of uSCION and SCION\n",
    "- experiment A : performance comparison of SCION/uSCION vs Adam/SGD/Muon over Fashion-MNIST\n",
    "- experiment B : hyperparameters transfer depending on the width on SVHN\n",
    "- experiment C : norm control SCG vs uSCG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a018873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17b48107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873568cd",
   "metadata": {},
   "source": [
    "## SCION / uSCION recap (implementation choices)\n",
    "\n",
    "We implement two variants:\n",
    "\n",
    "- **uSCION (unconstrained)**:  x <- x + γ * LMO(d)\n",
    "- **SCION (constrained / SCG-style)**: x <- (1-γ) x + γ * ρ * LMO(d)\n",
    "\n",
    "where:\n",
    "- d is a momentum-filtered stochastic gradient direction\n",
    "- LMO(d) returns an extreme point of the unit ball of a chosen norm (we implement:\n",
    "  - sign-LMO for vector-like parameters (bias, LayerNorm, etc.)\n",
    "  - spectral/polar-LMO for 2D matrix parameters\n",
    ")\n",
    "- ρ is an optional radius (default 1.0)\n",
    "\n",
    "Notes:\n",
    "- This notebook uses a *practical* per-parameter rule: matrices -> spectral/polar LMO, others -> sign LMO.\n",
    "- You can easily swap the LMO mapping if your paper uses a specific table of norms per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8216ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def lmo_sign(g: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    # returns argmin_{||s||_inf<=1} <s,g> = -sign(g)\n",
    "    return -torch.sign(g).clamp(min=-1.0, max=1.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def newton_schulz_polar_factor(G: torch.Tensor, iters: int = 5, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate the polar factor Q of matrix G:\n",
    "        G = Q H, with Q orthogonal-ish.\n",
    "    Uses Newton–Schulz iterations on a scaled matrix.\n",
    "    Returns Q (same shape as G).\n",
    "\n",
    "    Works best for reasonably-conditioned matrices; for small models/demos it's fine.\n",
    "    \"\"\"\n",
    "    assert G.ndim == 2\n",
    "    # scale by Fro norm to keep spectral radius in a good range\n",
    "    fro = torch.linalg.norm(G, ord=\"fro\")\n",
    "    if fro < eps:\n",
    "        return torch.zeros_like(G)\n",
    "\n",
    "    X = G / (fro + eps)\n",
    "    I = torch.eye(X.shape[0], device=X.device, dtype=X.dtype)\n",
    "\n",
    "    # If matrix is not square, work with \"tall\" case via left-polar:\n",
    "    # Q = X (X^T X)^(-1/2). Newton–Schulz typically assumes square;\n",
    "    # we handle rectangular by iterating on the smaller Gram matrix.\n",
    "    m, n = X.shape\n",
    "    if m >= n:\n",
    "        # iterate on A = X^T X (n x n)\n",
    "        A = X.T @ X\n",
    "        # normalize A\n",
    "        A = A / (torch.linalg.norm(A, ord=\"fro\") + eps)\n",
    "        Y = A\n",
    "        Z = torch.eye(n, device=X.device, dtype=X.dtype)\n",
    "        for _ in range(iters):\n",
    "            Tm = 0.5 * (3.0*torch.eye(n, device=X.device, dtype=X.dtype) - Z @ Y)\n",
    "            Y = Y @ Tm\n",
    "            Z = Tm @ Z\n",
    "        inv_sqrt = Z\n",
    "        Q = X @ inv_sqrt\n",
    "    else:\n",
    "        # wide: iterate on A = X X^T (m x m)\n",
    "        A = X @ X.T\n",
    "        A = A / (torch.linalg.norm(A, ord=\"fro\") + eps)\n",
    "        Y = A\n",
    "        Z = torch.eye(m, device=X.device, dtype=X.dtype)\n",
    "        for _ in range(iters):\n",
    "            Tm = 0.5 * (3.0*torch.eye(m, device=X.device, dtype=X.dtype) - Z @ Y)\n",
    "            Y = Y @ Tm\n",
    "            Z = Tm @ Z\n",
    "        inv_sqrt = Z\n",
    "        Q = inv_sqrt @ X\n",
    "\n",
    "    # LMO direction should minimize <S,G> over unit ball\n",
    "    # For the \"spectral/polar\" choice used in SCION-style methods, use -Q\n",
    "    return Q\n",
    "\n",
    "@torch.no_grad()\n",
    "def lmo_polar(G: torch.Tensor, iters: int = 5) -> torch.Tensor:\n",
    "    # return argmin <S,G> over \"orthogonal-ish\" extreme points ≈ -polar(G)\n",
    "    Q = newton_schulz_polar_factor(G, iters=iters)\n",
    "    return -Q\n",
    "\n",
    "@torch.no_grad()\n",
    "def lmo_dispatch(g: torch.Tensor, polar_iters: int = 5) -> torch.Tensor:\n",
    "    if g.ndim == 2:\n",
    "        return lmo_polar(g, iters=polar_iters)\n",
    "    else:\n",
    "        return lmo_sign(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28805cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCION(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    SCION / uSCION optimizer.\n",
    "\n",
    "    Params:\n",
    "      lr: γ\n",
    "      momentum: beta in [0,1)\n",
    "      constrained: if True, does convex combo (SCION/SCG), else additive (uSCION/uSCG)\n",
    "      radius: ρ (default 1.0) to scale the LMO output in constrained mode\n",
    "      polar_iters: Newton–Schulz iterations used for matrix LMO\n",
    "      weight_decay: optional (applied as decoupled WD, AdamW-style)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-2,\n",
    "        momentum: float = 0.9,\n",
    "        constrained: bool = False,\n",
    "        radius: float = 1.0,\n",
    "        polar_iters: int = 5,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        if lr <= 0:\n",
    "            raise ValueError(\"lr must be > 0\")\n",
    "        if not (0.0 <= momentum < 1.0):\n",
    "            raise ValueError(\"momentum must be in [0,1)\")\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            constrained=constrained,\n",
    "            radius=radius,\n",
    "            polar_iters=polar_iters,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta = group[\"momentum\"]\n",
    "            constrained = group[\"constrained\"]\n",
    "            rho = group[\"radius\"]\n",
    "            polar_iters = group[\"polar_iters\"]\n",
    "            wd = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                g = p.grad\n",
    "\n",
    "                # decoupled weight decay (optional)\n",
    "                if wd != 0.0:\n",
    "                    p.mul_(1.0 - lr * wd)\n",
    "\n",
    "                st = self.state[p]\n",
    "                if \"buf\" not in st:\n",
    "                    st[\"buf\"] = torch.zeros_like(g)\n",
    "\n",
    "                buf = st[\"buf\"]\n",
    "                # momentum direction (simple EMA)\n",
    "                buf.mul_(beta).add_(g, alpha=(1.0 - beta))\n",
    "                d = buf  # direction we feed to LMO\n",
    "\n",
    "                s = lmo_dispatch(d, polar_iters=polar_iters)\n",
    "\n",
    "                if constrained:\n",
    "                    # x <- (1-γ)x + γ * ρ*s\n",
    "                    p.mul_(1.0 - lr).add_(s, alpha=lr * rho)\n",
    "                else:\n",
    "                    # x <- x + γ * s\n",
    "                    p.add_(s, alpha=lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0998f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(name: str, model: nn.Module, lr: float, wd: float = 0.0, momentum: float = 0.9):\n",
    "    name = name.lower()\n",
    "\n",
    "    if name == \"adamw\":\n",
    "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd, nesterov=True)\n",
    "\n",
    "    if name == \"muon\":\n",
    "        # PyTorch has torch.optim.Muon in newer versions. Otherwise, fallback.\n",
    "        if hasattr(torch.optim, \"Muon\"):\n",
    "            # IMPORTANT: Muon is meant for 2D weight matrices; bias/embeddings typically use AdamW.\n",
    "            # Here we do a simple split: 2D -> Muon, rest -> AdamW\n",
    "            mat_params = []\n",
    "            other_params = []\n",
    "            for p in model.parameters():\n",
    "                if p.ndim == 2:\n",
    "                    mat_params.append(p)\n",
    "                else:\n",
    "                    other_params.append(p)\n",
    "\n",
    "            opt_muon = torch.optim.Muon(mat_params, lr=lr, weight_decay=wd)\n",
    "            opt_other = torch.optim.AdamW(other_params, lr=lr, weight_decay=wd)\n",
    "            return (opt_muon, opt_other)\n",
    "        else:\n",
    "            print(\"WARNING: torch.optim.Muon not found; falling back to AdamW.\")\n",
    "            return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    if name == \"scion\":\n",
    "        return SCION(model.parameters(), lr=lr, momentum=momentum, constrained=True, radius=1.0, weight_decay=wd)\n",
    "\n",
    "    if name == \"uscion\":\n",
    "        return SCION(model.parameters(), lr=lr, momentum=momentum, constrained=False, radius=1.0, weight_decay=wd)\n",
    "\n",
    "    raise ValueError(f\"Unknown optimizer: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16292a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, in_ch: int, num_classes: int, width: int = 64, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        w = width\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(w, w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(w, 2*w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(2*w, 2*w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*w, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "587e7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist(batch_size: int = 128, num_workers: int = 0):\n",
    "    tfm = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "    test  = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_svhn(batch_size: int = 128, num_workers: int = 0):\n",
    "    tfm = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    ])\n",
    "    train = torchvision.datasets.SVHN(root=\"./data\", split=\"train\", download=True, transform=tfm)\n",
    "    test  = torchvision.datasets.SVHN(root=\"./data\", split=\"test\",  download=True, transform=tfm)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2346b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(model, loader, optimizer, train: bool = True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if train:\n",
    "            # zero grad\n",
    "            if isinstance(optimizer, tuple):\n",
    "                for opt in optimizer:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            if isinstance(optimizer, tuple):\n",
    "                for opt in optimizer:\n",
    "                    opt.step()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += accuracy(logits, y) * bs\n",
    "        n += bs\n",
    "\n",
    "    return total_loss / n, total_acc / n\n",
    "\n",
    "def fit(model, train_loader, test_loader, optimizer, epochs: int = 10):\n",
    "    history = []\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, optimizer, train=True)\n",
    "        te_loss, te_acc = run_epoch(model, test_loader, optimizer, train=False)\n",
    "        dt = time.time() - t0\n",
    "        history.append(dict(epoch=ep, train_loss=tr_loss, train_acc=tr_acc, test_loss=te_loss, test_acc=te_acc, sec=dt))\n",
    "        print(f\"ep {ep:02d} | tr {tr_loss:.4f}/{tr_acc:.4f} | te {te_loss:.4f}/{te_acc:.4f} | {dt:.1f}s\")\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4dd7f",
   "metadata": {},
   "source": [
    "## Experiment A — Fashion-MNIST\n",
    "\n",
    "Compare:\n",
    "- AdamW\n",
    "- SGD (Nesterov)\n",
    "- Muon (if available in torch.optim)\n",
    "- SCION (constrained)\n",
    "- uSCION (unconstrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5ba63fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== adamw ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristan/Documents/dev/01_Cours_CS/06_ovo/SCION-method-review/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | tr 1.0309/0.6206 | te 0.6506/0.7601 | 13.7s\n",
      "ep 02 | tr 0.5885/0.7905 | te 0.5270/0.8171 | 11.0s\n",
      "ep 03 | tr 0.4842/0.8268 | te 0.4873/0.8209 | 11.3s\n",
      "ep 04 | tr 0.4261/0.8464 | te 0.3865/0.8611 | 12.0s\n",
      "ep 05 | tr 0.3816/0.8621 | te 0.3618/0.8738 | 11.9s\n",
      "ep 06 | tr 0.3551/0.8725 | te 0.3757/0.8688 | 11.1s\n",
      "ep 07 | tr 0.3351/0.8793 | te 0.3281/0.8845 | 11.1s\n",
      "ep 08 | tr 0.3154/0.8869 | te 0.3251/0.8856 | 12.4s\n",
      "ep 09 | tr 0.3046/0.8904 | te 0.3044/0.8920 | 12.6s\n",
      "ep 10 | tr 0.2928/0.8941 | te 0.2967/0.8950 | 11.5s\n",
      "\n",
      "=== sgd ===\n",
      "ep 01 | tr 1.2860/0.5164 | te 0.7170/0.7386 | 12.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, lr, wd \u001b[38;5;129;01min\u001b[39;00m configs:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m, name, \u001b[33m\"\u001b[39m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     dfs.append(\u001b[43mrun_expA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m dfA = pd.concat(dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m dfA.tail()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrun_expA\u001b[39m\u001b[34m(optim_name, lr, wd, momentum, width, epochs)\u001b[39m\n\u001b[32m      6\u001b[39m model = SmallCNN(in_ch=\u001b[32m1\u001b[39m, num_classes=\u001b[32m10\u001b[39m, width=width).to(device)\n\u001b[32m      7\u001b[39m opt = make_optimizer(optim_name, model, lr=lr, wd=wd, momentum=momentum)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m] = optim_name\n\u001b[32m     10\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m] = lr\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(model, train_loader, test_loader, optimizer, epochs)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     43\u001b[39m     t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     tr_loss, tr_acc = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     te_loss, te_acc = run_epoch(model, test_loader, optimizer, train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     46\u001b[39m     dt = time.time() - t0\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, loader, optimizer, train)\u001b[39m\n\u001b[32m     31\u001b[39m         optimizer.step()\n\u001b[32m     33\u001b[39m bs = x.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * bs\n\u001b[32m     35\u001b[39m total_acc  += accuracy(logits, y) * bs\n\u001b[32m     36\u001b[39m n += bs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "train_loader, test_loader = get_fashion_mnist(batch_size=256)\n",
    "\n",
    "def run_expA(optim_name: str, lr: float, wd: float = 0.0, momentum: float = 0.9, width: int = 64, epochs: int = 10):\n",
    "    model = SmallCNN(in_ch=1, num_classes=10, width=width).to(device)\n",
    "    opt = make_optimizer(optim_name, model, lr=lr, wd=wd, momentum=momentum)\n",
    "    df = fit(model, train_loader, test_loader, opt, epochs=epochs)\n",
    "    df[\"optimizer\"] = optim_name\n",
    "    df[\"lr\"] = lr\n",
    "    df[\"wd\"] = wd\n",
    "    df[\"width\"] = width\n",
    "    return df\n",
    "\n",
    "# Reasonable starting points (adjust if needed)\n",
    "configs = [\n",
    "    (\"adamw\",  1e-3, 1e-4),\n",
    "    (\"sgd\",    5e-2, 5e-4),\n",
    "    (\"muon\",   2e-3, 1e-4),\n",
    "    (\"scion\",  5e-2, 0.0),\n",
    "    (\"uscion\", 5e-2, 0.0),\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for name, lr, wd in configs:\n",
    "    print(\"\\n===\", name, \"===\")\n",
    "    dfs.append(run_expA(name, lr=lr, wd=wd, epochs=10))\n",
    "\n",
    "dfA = pd.concat(dfs, ignore_index=True)\n",
    "dfA.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(df, metric: str, title: str):\n",
    "    plt.figure()\n",
    "    for opt_name, sub in df.groupby(\"optimizer\"):\n",
    "        plt.plot(sub[\"epoch\"], sub[metric], label=opt_name)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_metric(dfA, \"test_acc\",  \"Experiment A — FashionMNIST test accuracy\")\n",
    "plot_metric(dfA, \"test_loss\", \"Experiment A — FashionMNIST test loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4fadb",
   "metadata": {},
   "source": [
    "## Experiment B — SVHN hyperparameter transfer across widths\n",
    "\n",
    "Protocol:\n",
    "1) Choose a base width (e.g., 64) and tune γ (learning rate) on it.\n",
    "2) Reuse the best γ for larger widths (e.g., 128, 256).\n",
    "3) Compare how stable the transferred hyperparameter is (especially for SCION/uSCION vs AdamW/SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "svhn_train, svhn_test = get_svhn(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d982f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once_svhn(optim_name: str, lr: float, width: int, epochs: int = 5, wd: float = 0.0):\n",
    "    model = SmallCNN(in_ch=3, num_classes=10, width=width).to(device)\n",
    "    opt = make_optimizer(optim_name, model, lr=lr, wd=wd)\n",
    "    df = fit(model, svhn_train, svhn_test, opt, epochs=epochs)\n",
    "    df[\"optimizer\"] = optim_name\n",
    "    df[\"lr\"] = lr\n",
    "    df[\"width\"] = width\n",
    "    df[\"wd\"] = wd\n",
    "    return df\n",
    "\n",
    "base_width = 64\n",
    "grid = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]  # keep small; SVHN is harder\n",
    "\n",
    "optimizers_B = [\"adamw\", \"sgd\", \"scion\", \"uscion\"]  # include muon if you want too\n",
    "\n",
    "dfs = []\n",
    "for opt_name in optimizers_B:\n",
    "    for lr in grid:\n",
    "        print(f\"\\n[SVHN tune] opt={opt_name} width={base_width} lr={lr}\")\n",
    "        dfs.append(run_once_svhn(opt_name, lr=lr, width=base_width, epochs=5, wd=1e-4 if opt_name==\"adamw\" else 0.0))\n",
    "\n",
    "dfB_tune = pd.concat(dfs, ignore_index=True)\n",
    "dfB_tune.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9df261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick best lr by best test_acc at last epoch\n",
    "last = dfB_tune.sort_values(\"epoch\").groupby([\"optimizer\",\"lr\",\"width\"]).tail(1)\n",
    "best = last.sort_values(\"test_acc\", ascending=False).groupby(\"optimizer\").head(1)\n",
    "best_lrs = {row[\"optimizer\"]: float(row[\"lr\"]) for _, row in best.iterrows()}\n",
    "best_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ca548",
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [64, 128, 256]\n",
    "dfs = []\n",
    "\n",
    "for opt_name in optimizers_B:\n",
    "    lr = best_lrs[opt_name]\n",
    "    for w in widths:\n",
    "        print(f\"\\n[SVHN transfer] opt={opt_name} width={w} lr={lr}\")\n",
    "        dfs.append(run_once_svhn(opt_name, lr=lr, width=w, epochs=10, wd=1e-4 if opt_name==\"adamw\" else 0.0))\n",
    "\n",
    "dfB = pd.concat(dfs, ignore_index=True)\n",
    "dfB.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc62fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for (opt_name, w), sub in dfB.groupby([\"optimizer\",\"width\"]):\n",
    "    sub_last = sub.sort_values(\"epoch\").groupby([\"optimizer\",\"width\"]).tail(1)\n",
    "    # We'll plot last-epoch accuracy as points\n",
    "for opt_name in optimizers_B:\n",
    "    sub_last = dfB[dfB[\"optimizer\"]==opt_name].sort_values(\"epoch\").groupby([\"optimizer\",\"width\"]).tail(1)\n",
    "    plt.plot(sub_last[\"width\"], sub_last[\"test_acc\"], marker=\"o\", label=opt_name)\n",
    "\n",
    "plt.xlabel(\"width\")\n",
    "plt.ylabel(\"test_acc (last epoch)\")\n",
    "plt.title(\"Experiment B — SVHN hyperparam transfer across widths (best lr @ width=64)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f46e7",
   "metadata": {},
   "source": [
    "## Experiment C — Norm control: constrained (SCION/SCG) vs unconstrained (uSCION/uSCG)\n",
    "\n",
    "We log per-epoch statistics of weight matrices, e.g.:\n",
    "- spectral norm (approx via power iteration)\n",
    "- Frobenius norm\n",
    "and compare trajectories for SCION vs uSCION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def power_iteration_spectral_norm(W: torch.Tensor, iters: int = 10, eps: float = 1e-12) -> float:\n",
    "    # approximate ||W||_2 for 2D tensor\n",
    "    if W.ndim != 2:\n",
    "        return float(\"nan\")\n",
    "    m, n = W.shape\n",
    "    v = torch.randn(n, device=W.device, dtype=W.dtype)\n",
    "    v = v / (v.norm() + eps)\n",
    "    for _ in range(iters):\n",
    "        u = W @ v\n",
    "        u = u / (u.norm() + eps)\n",
    "        v = W.T @ u\n",
    "        v = v / (v.norm() + eps)\n",
    "    sigma = (u @ (W @ v)).abs().item()\n",
    "    return sigma\n",
    "\n",
    "@torch.no_grad()\n",
    "def model_norm_stats(model: nn.Module) -> Dict[str, float]:\n",
    "    specs = []\n",
    "    fros = []\n",
    "    for p in model.parameters():\n",
    "        if p.ndim == 2:\n",
    "            specs.append(power_iteration_spectral_norm(p))\n",
    "            fros.append(torch.linalg.norm(p, ord=\"fro\").item())\n",
    "    out = {}\n",
    "    if len(specs) > 0:\n",
    "        out[\"spec_mean\"] = float(np.mean(specs))\n",
    "        out[\"spec_max\"]  = float(np.max(specs))\n",
    "        out[\"fro_mean\"]  = float(np.mean(fros))\n",
    "        out[\"fro_max\"]   = float(np.max(fros))\n",
    "    else:\n",
    "        out[\"spec_mean\"] = out[\"spec_max\"] = out[\"fro_mean\"] = out[\"fro_max\"] = float(\"nan\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76103cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_norm_logs(model, train_loader, test_loader, optimizer, epochs: int = 10):\n",
    "    history = []\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, optimizer, train=True)\n",
    "        te_loss, te_acc = run_epoch(model, test_loader, optimizer, train=False)\n",
    "        stats = model_norm_stats(model)\n",
    "        dt = time.time() - t0\n",
    "        row = dict(epoch=ep, train_loss=tr_loss, train_acc=tr_acc, test_loss=te_loss, test_acc=te_acc, sec=dt, **stats)\n",
    "        history.append(row)\n",
    "        print(f\"ep {ep:02d} | te_acc {te_acc:.4f} | spec_max {stats['spec_max']:.3f} | {dt:.1f}s\")\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8161ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)\n",
    "train_loader, test_loader = get_fashion_mnist(batch_size=256)\n",
    "\n",
    "def run_expC(constrained: bool, lr: float = 5e-2, width: int = 64, epochs: int = 10):\n",
    "    model = SmallCNN(in_ch=1, num_classes=10, width=width).to(device)\n",
    "    opt = SCION(model.parameters(), lr=lr, momentum=0.9, constrained=constrained, radius=1.0, weight_decay=0.0)\n",
    "    df = fit_with_norm_logs(model, train_loader, test_loader, opt, epochs=epochs)\n",
    "    df[\"variant\"] = \"SCION(constrained)\" if constrained else \"uSCION(unconstrained)\"\n",
    "    df[\"lr\"] = lr\n",
    "    df[\"width\"] = width\n",
    "    return df\n",
    "\n",
    "dfC1 = run_expC(constrained=True,  lr=5e-2, epochs=10)\n",
    "dfC2 = run_expC(constrained=False, lr=5e-2, epochs=10)\n",
    "dfC = pd.concat([dfC1, dfC2], ignore_index=True)\n",
    "dfC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norm(df, col: str, title: str):\n",
    "    plt.figure()\n",
    "    for name, sub in df.groupby(\"variant\"):\n",
    "        plt.plot(sub[\"epoch\"], sub[col], label=name)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(col)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_norm(dfC, \"spec_max\", \"Experiment C — spectral norm max (matrices)\")\n",
    "plot_norm(dfC, \"fro_max\",  \"Experiment C — Frobenius norm max (matrices)\")\n",
    "plot_metric(dfC.rename(columns={\"variant\":\"optimizer\"}), \"test_acc\", \"Experiment C — test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43e015",
   "metadata": {},
   "source": [
    "## Checklist for submission\n",
    "\n",
    "- [ ] uSCION and SCION implementation included\n",
    "- [ ] Experiment A: Fashion-MNIST comparison vs Adam/SGD/Muon\n",
    "- [ ] Experiment B: SVHN hyperparameter transfer across widths\n",
    "- [ ] Experiment C: Norm control constrained vs unconstrained (norm logs + plot)\n",
    "- [ ] Run on data not used in the original paper (Fashion-MNIST + SVHN satisfy this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb1071",
   "metadata": {},
   "source": [
    "# EXPERIMENT D — MiniGPT + WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef40fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# ---- Tokenization ----\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocab():\n",
    "    counter = Counter()\n",
    "    for line in WikiText2(split=\"train\"):\n",
    "        counter.update(tokenizer(line))\n",
    "    v = vocab(counter, specials=[\"<unk>\"])\n",
    "    v.set_default_index(v[\"<unk>\"])\n",
    "    return v\n",
    "\n",
    "vocab_obj = build_vocab()\n",
    "vocab_size = len(vocab_obj)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "    data = []\n",
    "    for item in raw_text_iter:\n",
    "        tokens = tokenizer(item)\n",
    "        ids = [vocab_obj[token] for token in tokens]\n",
    "        if len(ids) > 0:\n",
    "            data.extend(ids)\n",
    "    return torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "train_data = data_process(WikiText2(split=\"train\"))\n",
    "val_data   = data_process(WikiText2(split=\"valid\"))\n",
    "test_data  = data_process(WikiText2(split=\"test\"))\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    seq_len = data.size(0) // batch_size\n",
    "    data = data[:seq_len * batch_size]\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "def get_batch(source, i, seq_len=128):\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=6, dim_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, 1024, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.tok_emb(x)\n",
    "        pos = self.pos_emb[:, :T, :]\n",
    "        h = tok + pos\n",
    "        \n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        h = self.transformer(h, mask=mask)\n",
    "        h = self.ln(h)\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gpt(model, data_source, batch_size=32, seq_len=128):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = vocab_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0)-1, seq_len):\n",
    "            data, targets = get_batch(data_source, i, seq_len)\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(data.T)\n",
    "            loss = F.cross_entropy(\n",
    "                output.reshape(-1, ntokens),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "    return math.exp(total_loss / (data_source.size(0) // seq_len))\n",
    "\n",
    "\n",
    "def train_gpt(model, train_data, val_data, optimizer, epochs=3, batch_size=32, seq_len=128):\n",
    "    train_data_b = batchify(train_data, batch_size).to(device)\n",
    "    val_data_b   = batchify(val_data, batch_size).to(device)\n",
    "    \n",
    "    history = []\n",
    "    ntokens = vocab_size\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        \n",
    "        for i in range(0, train_data_b.size(0)-1, seq_len):\n",
    "            data, targets = get_batch(train_data_b, i, seq_len)\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.T)\n",
    "            loss = F.cross_entropy(\n",
    "                output.reshape(-1, ntokens),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        val_ppl = evaluate_gpt(model, val_data_b, batch_size, seq_len)\n",
    "        print(f\"Epoch {ep} | Val Perplexity: {val_ppl:.2f}\")\n",
    "        history.append(val_ppl)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33076df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "\n",
    "model_adam = MiniGPT(vocab_size).to(device)\n",
    "opt_adam = torch.optim.AdamW(model_adam.parameters(), lr=3e-4)\n",
    "\n",
    "print(\"Training Adam...\")\n",
    "hist_adam = train_gpt(model_adam, train_data, val_data, opt_adam, epochs=3)\n",
    "\n",
    "model_scion = MiniGPT(vocab_size).to(device)\n",
    "opt_scion = SCION(model_scion.parameters(), lr=5e-3, constrained=True)\n",
    "\n",
    "print(\"Training SCION...\")\n",
    "hist_scion = train_gpt(model_scion, train_data, val_data, opt_scion, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist_adam, label=\"AdamW\")\n",
    "plt.plot(hist_scion, label=\"SCION\")\n",
    "plt.ylabel(\"Val Perplexity\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.title(\"MiniGPT on WikiText2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "results_batch = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\nBatch size {bs} — SCION\")\n",
    "    model = MiniGPT(vocab_size).to(device)\n",
    "    opt = SCION(model.parameters(), lr=5e-3, constrained=True)\n",
    "    hist = train_gpt(model, train_data, val_data, opt, epochs=2, batch_size=bs)\n",
    "    results_batch[bs] = hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb481a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [128, 256, 384]\n",
    "lr_scion = 5e-3\n",
    "\n",
    "transfer_results = {}\n",
    "\n",
    "for d_model in widths:\n",
    "    print(f\"\\nWidth {d_model}\")\n",
    "    model = MiniGPT(vocab_size, d_model=d_model).to(device)\n",
    "    opt = SCION(model.parameters(), lr=lr_scion, constrained=True)\n",
    "    hist = train_gpt(model, train_data, val_data, opt, epochs=2)\n",
    "    transfer_results[d_model] = hist[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scion-method-review (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
