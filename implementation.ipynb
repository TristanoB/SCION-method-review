{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed89b3b",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51e181",
   "metadata": {},
   "source": [
    "This notebook is structured in the following manner : \n",
    "- implementation of uSCION and SCION\n",
    "- experiment A : performance comparison of SCION/uSCION vs Adam/SGD/Muon over Fashion-MNIST\n",
    "- experiment B : hyperparameters transfer depending on the width on SVHN\n",
    "- experiment C : norm control SCG vs uSCG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a018873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17b48107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873568cd",
   "metadata": {},
   "source": [
    "## SCION / uSCION recap (implementation choices)\n",
    "\n",
    "We implement two variants:\n",
    "\n",
    "- **uSCION (unconstrained)**:  x <- x + γ * LMO(d)\n",
    "- **SCION (constrained / SCG-style)**: x <- (1-γ) x + γ * ρ * LMO(d)\n",
    "\n",
    "where:\n",
    "- d is a momentum-filtered stochastic gradient direction\n",
    "- LMO(d) returns an extreme point of the unit ball of a chosen norm (we implement:\n",
    "  - sign-LMO for vector-like parameters (bias, LayerNorm, etc.)\n",
    "  - spectral/polar-LMO for 2D matrix parameters\n",
    ")\n",
    "- ρ is an optional radius (default 1.0)\n",
    "\n",
    "Notes:\n",
    "- This notebook uses a *practical* per-parameter rule: matrices -> spectral/polar LMO, others -> sign LMO.\n",
    "- You can easily swap the LMO mapping if your paper uses a specific table of norms per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8216ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def lmo_sign(g: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    # returns argmin_{||s||_inf<=1} <s,g> = -sign(g)\n",
    "    return -torch.sign(g).clamp(min=-1.0, max=1.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def newton_schulz_polar_factor(G: torch.Tensor, iters: int = 5, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate the polar factor Q of matrix G:\n",
    "        G = Q H, with Q orthogonal-ish.\n",
    "    Uses Newton–Schulz iterations on a scaled matrix.\n",
    "    Returns Q (same shape as G).\n",
    "\n",
    "    Works best for reasonably-conditioned matrices; for small models/demos it's fine.\n",
    "    \"\"\"\n",
    "    assert G.ndim == 2\n",
    "    # scale by Fro norm to keep spectral radius in a good range\n",
    "    fro = torch.linalg.norm(G, ord=\"fro\")\n",
    "    if fro < eps:\n",
    "        return torch.zeros_like(G)\n",
    "\n",
    "    X = G / (fro + eps)\n",
    "    I = torch.eye(X.shape[0], device=X.device, dtype=X.dtype)\n",
    "\n",
    "    # If matrix is not square, work with \"tall\" case via left-polar:\n",
    "    # Q = X (X^T X)^(-1/2). Newton–Schulz typically assumes square;\n",
    "    # we handle rectangular by iterating on the smaller Gram matrix.\n",
    "    m, n = X.shape\n",
    "    if m >= n:\n",
    "        # iterate on A = X^T X (n x n)\n",
    "        A = X.T @ X\n",
    "        # normalize A\n",
    "        A = A / (torch.linalg.norm(A, ord=\"fro\") + eps)\n",
    "        Y = A\n",
    "        Z = torch.eye(n, device=X.device, dtype=X.dtype)\n",
    "        for _ in range(iters):\n",
    "            Tm = 0.5 * (3.0*torch.eye(n, device=X.device, dtype=X.dtype) - Z @ Y)\n",
    "            Y = Y @ Tm\n",
    "            Z = Tm @ Z\n",
    "        inv_sqrt = Z\n",
    "        Q = X @ inv_sqrt\n",
    "    else:\n",
    "        # wide: iterate on A = X X^T (m x m)\n",
    "        A = X @ X.T\n",
    "        A = A / (torch.linalg.norm(A, ord=\"fro\") + eps)\n",
    "        Y = A\n",
    "        Z = torch.eye(m, device=X.device, dtype=X.dtype)\n",
    "        for _ in range(iters):\n",
    "            Tm = 0.5 * (3.0*torch.eye(m, device=X.device, dtype=X.dtype) - Z @ Y)\n",
    "            Y = Y @ Tm\n",
    "            Z = Tm @ Z\n",
    "        inv_sqrt = Z\n",
    "        Q = inv_sqrt @ X\n",
    "\n",
    "    # LMO direction should minimize <S,G> over unit ball\n",
    "    # For the \"spectral/polar\" choice used in SCION-style methods, use -Q\n",
    "    return Q\n",
    "\n",
    "@torch.no_grad()\n",
    "def lmo_polar(G: torch.Tensor, iters: int = 5) -> torch.Tensor:\n",
    "    # return argmin <S,G> over \"orthogonal-ish\" extreme points ≈ -polar(G)\n",
    "    Q = newton_schulz_polar_factor(G, iters=iters)\n",
    "    return -Q\n",
    "\n",
    "@torch.no_grad()\n",
    "def lmo_dispatch(g: torch.Tensor, polar_iters: int = 5) -> torch.Tensor:\n",
    "    if g.ndim == 2:\n",
    "        return lmo_polar(g, iters=polar_iters)\n",
    "    else:\n",
    "        return lmo_sign(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28805cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCION(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    SCION / uSCION optimizer.\n",
    "\n",
    "    Params:\n",
    "      lr: γ\n",
    "      momentum: beta in [0,1)\n",
    "      constrained: if True, does convex combo (SCION/SCG), else additive (uSCION/uSCG)\n",
    "      radius: ρ (default 1.0) to scale the LMO output in constrained mode\n",
    "      polar_iters: Newton–Schulz iterations used for matrix LMO\n",
    "      weight_decay: optional (applied as decoupled WD, AdamW-style)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-2,\n",
    "        momentum: float = 0.9,\n",
    "        constrained: bool = False,\n",
    "        radius: float = 1.0,\n",
    "        polar_iters: int = 5,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        if lr <= 0:\n",
    "            raise ValueError(\"lr must be > 0\")\n",
    "        if not (0.0 <= momentum < 1.0):\n",
    "            raise ValueError(\"momentum must be in [0,1)\")\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            constrained=constrained,\n",
    "            radius=radius,\n",
    "            polar_iters=polar_iters,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta = group[\"momentum\"]\n",
    "            constrained = group[\"constrained\"]\n",
    "            rho = group[\"radius\"]\n",
    "            polar_iters = group[\"polar_iters\"]\n",
    "            wd = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                g = p.grad\n",
    "\n",
    "                # decoupled weight decay (optional)\n",
    "                if wd != 0.0:\n",
    "                    p.mul_(1.0 - lr * wd)\n",
    "\n",
    "                st = self.state[p]\n",
    "                if \"buf\" not in st:\n",
    "                    st[\"buf\"] = torch.zeros_like(g)\n",
    "\n",
    "                buf = st[\"buf\"]\n",
    "                # momentum direction (simple EMA)\n",
    "                buf.mul_(beta).add_(g, alpha=(1.0 - beta))\n",
    "                d = buf  # direction we feed to LMO\n",
    "\n",
    "                s = lmo_dispatch(d, polar_iters=polar_iters)\n",
    "\n",
    "                if constrained:\n",
    "                    # x <- (1-γ)x + γ * ρ*s\n",
    "                    p.mul_(1.0 - lr).add_(s, alpha=lr * rho)\n",
    "                else:\n",
    "                    # x <- x + γ * s\n",
    "                    p.add_(s, alpha=lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0998f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(name: str, model: nn.Module, lr: float, wd: float = 0.0, momentum: float = 0.9):\n",
    "    name = name.lower()\n",
    "\n",
    "    if name == \"adamw\":\n",
    "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd, nesterov=True)\n",
    "\n",
    "    if name == \"muon\":\n",
    "        # PyTorch has torch.optim.Muon in newer versions. Otherwise, fallback.\n",
    "        if hasattr(torch.optim, \"Muon\"):\n",
    "            # IMPORTANT: Muon is meant for 2D weight matrices; bias/embeddings typically use AdamW.\n",
    "            # Here we do a simple split: 2D -> Muon, rest -> AdamW\n",
    "            mat_params = []\n",
    "            other_params = []\n",
    "            for p in model.parameters():\n",
    "                if p.ndim == 2:\n",
    "                    mat_params.append(p)\n",
    "                else:\n",
    "                    other_params.append(p)\n",
    "\n",
    "            opt_muon = torch.optim.Muon(mat_params, lr=lr, weight_decay=wd)\n",
    "            opt_other = torch.optim.AdamW(other_params, lr=lr, weight_decay=wd)\n",
    "            return (opt_muon, opt_other)\n",
    "        else:\n",
    "            print(\"WARNING: torch.optim.Muon not found; falling back to AdamW.\")\n",
    "            return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    if name == \"scion\":\n",
    "        return SCION(model.parameters(), lr=lr, momentum=momentum, constrained=True, radius=1.0, weight_decay=wd)\n",
    "\n",
    "    if name == \"uscion\":\n",
    "        return SCION(model.parameters(), lr=lr, momentum=momentum, constrained=False, radius=1.0, weight_decay=wd)\n",
    "\n",
    "    raise ValueError(f\"Unknown optimizer: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16292a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, in_ch: int, num_classes: int, width: int = 64, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        w = width\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(w, w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(w, 2*w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(2*w, 2*w, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*w, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "587e7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist(batch_size: int = 128, num_workers: int = 0):\n",
    "    tfm = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "    test  = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_svhn(batch_size: int = 128, num_workers: int = 0):\n",
    "    tfm = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    ])\n",
    "    train = torchvision.datasets.SVHN(root=\"./data\", split=\"train\", download=True, transform=tfm)\n",
    "    test  = torchvision.datasets.SVHN(root=\"./data\", split=\"test\",  download=True, transform=tfm)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2346b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(model, loader, optimizer, train: bool = True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if train:\n",
    "            # zero grad\n",
    "            if isinstance(optimizer, tuple):\n",
    "                for opt in optimizer:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            if isinstance(optimizer, tuple):\n",
    "                for opt in optimizer:\n",
    "                    opt.step()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += accuracy(logits, y) * bs\n",
    "        n += bs\n",
    "\n",
    "    return total_loss / n, total_acc / n\n",
    "\n",
    "def fit(model, train_loader, test_loader, optimizer, epochs: int = 10):\n",
    "    history = []\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, optimizer, train=True)\n",
    "        te_loss, te_acc = run_epoch(model, test_loader, optimizer, train=False)\n",
    "        dt = time.time() - t0\n",
    "        history.append(dict(epoch=ep, train_loss=tr_loss, train_acc=tr_acc, test_loss=te_loss, test_acc=te_acc, sec=dt))\n",
    "        print(f\"ep {ep:02d} | tr {tr_loss:.4f}/{tr_acc:.4f} | te {te_loss:.4f}/{te_acc:.4f} | {dt:.1f}s\")\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4dd7f",
   "metadata": {},
   "source": [
    "## Experiment A — Fashion-MNIST\n",
    "\n",
    "Compare:\n",
    "- AdamW\n",
    "- SGD (Nesterov)\n",
    "- Muon (if available in torch.optim)\n",
    "- SCION (constrained)\n",
    "- uSCION (unconstrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5ba63fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== adamw ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristan/Documents/dev/01_Cours_CS/06_ovo/SCION-method-review/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | tr 1.0309/0.6206 | te 0.6506/0.7601 | 13.7s\n",
      "ep 02 | tr 0.5885/0.7905 | te 0.5270/0.8171 | 11.0s\n",
      "ep 03 | tr 0.4842/0.8268 | te 0.4873/0.8209 | 11.3s\n",
      "ep 04 | tr 0.4261/0.8464 | te 0.3865/0.8611 | 12.0s\n",
      "ep 05 | tr 0.3816/0.8621 | te 0.3618/0.8738 | 11.9s\n",
      "ep 06 | tr 0.3551/0.8725 | te 0.3757/0.8688 | 11.1s\n",
      "ep 07 | tr 0.3351/0.8793 | te 0.3281/0.8845 | 11.1s\n",
      "ep 08 | tr 0.3154/0.8869 | te 0.3251/0.8856 | 12.4s\n",
      "ep 09 | tr 0.3046/0.8904 | te 0.3044/0.8920 | 12.6s\n",
      "ep 10 | tr 0.2928/0.8941 | te 0.2967/0.8950 | 11.5s\n",
      "\n",
      "=== sgd ===\n",
      "ep 01 | tr 1.2860/0.5164 | te 0.7170/0.7386 | 12.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, lr, wd \u001b[38;5;129;01min\u001b[39;00m configs:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m, name, \u001b[33m\"\u001b[39m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     dfs.append(\u001b[43mrun_expA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m dfA = pd.concat(dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m dfA.tail()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrun_expA\u001b[39m\u001b[34m(optim_name, lr, wd, momentum, width, epochs)\u001b[39m\n\u001b[32m      6\u001b[39m model = SmallCNN(in_ch=\u001b[32m1\u001b[39m, num_classes=\u001b[32m10\u001b[39m, width=width).to(device)\n\u001b[32m      7\u001b[39m opt = make_optimizer(optim_name, model, lr=lr, wd=wd, momentum=momentum)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m] = optim_name\n\u001b[32m     10\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m] = lr\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(model, train_loader, test_loader, optimizer, epochs)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     43\u001b[39m     t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     tr_loss, tr_acc = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     te_loss, te_acc = run_epoch(model, test_loader, optimizer, train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     46\u001b[39m     dt = time.time() - t0\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, loader, optimizer, train)\u001b[39m\n\u001b[32m     31\u001b[39m         optimizer.step()\n\u001b[32m     33\u001b[39m bs = x.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * bs\n\u001b[32m     35\u001b[39m total_acc  += accuracy(logits, y) * bs\n\u001b[32m     36\u001b[39m n += bs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "train_loader, test_loader = get_fashion_mnist(batch_size=256)\n",
    "\n",
    "def run_expA(optim_name: str, lr: float, wd: float = 0.0, momentum: float = 0.9, width: int = 64, epochs: int = 10):\n",
    "    model = SmallCNN(in_ch=1, num_classes=10, width=width).to(device)\n",
    "    opt = make_optimizer(optim_name, model, lr=lr, wd=wd, momentum=momentum)\n",
    "    df = fit(model, train_loader, test_loader, opt, epochs=epochs)\n",
    "    df[\"optimizer\"] = optim_name\n",
    "    df[\"lr\"] = lr\n",
    "    df[\"wd\"] = wd\n",
    "    df[\"width\"] = width\n",
    "    return df\n",
    "\n",
    "# Reasonable starting points (adjust if needed)\n",
    "configs = [\n",
    "    (\"adamw\",  1e-3, 1e-4),\n",
    "    (\"sgd\",    5e-2, 5e-4),\n",
    "    (\"muon\",   2e-3, 1e-4),\n",
    "    (\"scion\",  5e-2, 0.0),\n",
    "    (\"uscion\", 5e-2, 0.0),\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for name, lr, wd in configs:\n",
    "    print(\"\\n===\", name, \"===\")\n",
    "    dfs.append(run_expA(name, lr=lr, wd=wd, epochs=10))\n",
    "\n",
    "dfA = pd.concat(dfs, ignore_index=True)\n",
    "dfA.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scion-method-review (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
